{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67a03366-64a8-41a3-93ee-6d2a8d29e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from ollama import Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a8dffbf-fa71-49b2-939a-f93ffa883c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction complete. Found documents: ['EA_Syllabus_NLPAI_Dan_FINAL.pdf']\n"
     ]
    }
   ],
   "source": [
    "# Use PyPDF2 to extract text from the PDFs\n",
    "# Function to extract text from a PDF file\n",
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = ''\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "    \n",
    "# Directory where PDF files are located\n",
    "pdf_dir = '/Users/joaoreis/Documents/GitHub/myChatbot/myPDF'\n",
    "pdf_texts = []\n",
    "pdf_filenames = []\n",
    "\n",
    "# Extract text from each PDF and store the filenames\n",
    "for pdf_file in os.listdir(pdf_dir):\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        pdf_text = extract_text_from_pdf(pdf_path)\n",
    "        pdf_texts.append(pdf_text)\n",
    "        pdf_filenames.append(pdf_file)\n",
    "\n",
    "print(\"Text extraction complete. Found documents:\", pdf_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0ca69f3-7b48-4634-abb8-e69e37db4c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using SentenceTransformers\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for the extracted text\n",
    "#pdf_embeddings = embedding_model.embed_documents(pdf_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1879bea1-326b-4d63-b984-f96cf342b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embeddings in ChromaDB using LangChain\n",
    "# Use LangChain’s Chroma class to interact with ChromaDB\n",
    "# Initialize ChromaDB client and collection using LangChain's Chroma\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    collection_name=\"pdf_documents\",\n",
    "    persist_directory=\"./chromadb\"\n",
    ")\n",
    "\n",
    "# Add the documents to ChromaDB\n",
    "for i, text in enumerate(pdf_texts):\n",
    "    vectorstore.add_texts(texts=[text], ids=[pdf_filenames[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "817776bb-cfad-45d8-85eb-f0c57e5bcfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    }
   ],
   "source": [
    "# Use LangChain’s RetrievalQA with Ollama\n",
    "# We will use LangChain’s RetrievalQA chain and Ollama’s Client for querying the LLaMA3.1 model.\n",
    "# Initialize the Ollama client\n",
    "\n",
    "# Initialize Ollama client\n",
    "ollama_client = Client()\n",
    "\n",
    "# Function to query LLaMA3 via Ollama's Client\n",
    "#def get_llama3_response(prompt):\n",
    "#    response = ollama_client.generate(model=\"llama3.1\", prompt=prompt)\n",
    "#    return response['text']\n",
    "\n",
    "# Example user question\n",
    "user_question = \"What is the class program?\"\n",
    "\n",
    "# Query ChromaDB using the proper LangChain API\n",
    "results = vectorstore.similarity_search(query=user_question, k=3)  # k=3 means top 3 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f233c374-16ca-4e68-a4bb-4b9666a0ec4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the provided text, it appears to be a course syllabus or outline for an online class. Here's an overview of the class:\n",
      "\n",
      "**Class Title:** (Not specified)\n",
      "\n",
      "**Duration:** The class seems to span several days in December 2024.\n",
      "\n",
      "**Instructor:** Daniel Dan, External Lecturer at the Institute for Marketing and Customer Analytics, Vienna University of Economics and Business.\n",
      "\n",
      "**Course Description:**\n",
      "\n",
      "The course appears to cover Natural Language Processing (NLP) concepts, including Topic Modeling, Sentiment Analysis, Named Entity Recognition, Word Embeddings, Transformers, and chatbot development. Students will learn how to create their own chatbots using the knowledge gained during the class.\n",
      "\n",
      "**Assessment Breakdown:**\n",
      "\n",
      "* Pre-module assignment: 30%\n",
      "* Core module (class participation and assignments): 30%\n",
      "* Post-module assignment (group project presentation): 40%\n",
      "\n",
      "**Submission Guidelines:**\n",
      "\n",
      "All student work is checked for plagiarism and AI-generated content. Submissions must be made as PDF files through the Moodle platform, with a specific naming convention.\n",
      "\n",
      "Please note that this summary is based on the provided text and might not be an exhaustive representation of the class program or its requirements.\n",
      "\n",
      "Retrieved documents:\n",
      "Document 1 content:  \n",
      "1 \n",
      "www.executiveacademy.at  Program(s):     (Executive) MBA DD & \n",
      "     Data  Science  certificate  program  \n",
      "Syllabus  for Module:  Natural  Language  Processing  & AI \n",
      " \n",
      "Date:   \n",
      "December  11-12, 2...\n",
      "\n",
      "PDF texts content:\n",
      "PDF 1 content:  \n",
      "1 \n",
      "www.executiveacademy.at  Program(s):     (Executive) MBA DD & \n",
      "     Data  Science  certificate  program  \n",
      "Syllabus  for Module:  Natural  Language  Processing  & AI \n",
      " \n",
      "Date:   \n",
      "December  11-12, 2...\n"
     ]
    }
   ],
   "source": [
    "# Query ChromaDB for relevant documents\n",
    "# Example user question\n",
    "\n",
    "# Process and generate a response using Ollama\n",
    "if results:\n",
    "    prompt = \"You are an assistant. Use the following documents to answer the question:\\n\"\n",
    "    for result in results:\n",
    "        prompt += f\"{result.page_content}\\n\"\n",
    "    prompt += f\"\\nAnswer the following question: {user_question}\"\n",
    "\n",
    "    # Get the response from the Ollama model\n",
    "    response = ollama_client.generate(model=\"llama3.1\", prompt=prompt)\n",
    "    if 'response' in response:\n",
    "        print(f\"Answer: {response['response']}\")\n",
    "    else:\n",
    "        print(\"No 'response' key found in the response.\")\n",
    "else:\n",
    "    print(\"No relevant information found.\")\n",
    "\n",
    "# Check if documents are retrieved\n",
    "if results:\n",
    "    print(\"\\nRetrieved documents:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"Document {i} content: {result.page_content[:200]}...\")  # Print first 200 characters\n",
    "else:\n",
    "    print(\"No relevant documents found.\")\n",
    "\n",
    "print(\"\\nPDF texts content:\")\n",
    "for i, text in enumerate(pdf_texts, 1):\n",
    "    print(f\"PDF {i} content: {text[:200]}...\")  # Print first 200 characters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
